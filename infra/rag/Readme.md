> **rag_this is a quick python based RAG accelerator**  built from a combination of code based on [liamca/GPT4oContentExtraction: Using Azure OpenAI GPT 4o to extract information such as text, tables and charts from Documents to Markdown](https://github.com/liamca/GPT4oContentExtraction) and the **Content Understanding** sample code  from this repo: [Azure-Samples/azure-ai-content-understanding-python](https://github.com/Azure-Samples/azure-ai-content-understanding-python)
> 
> Input can be individual .docx or pdf files (other formats like pptx may work but have not been tested)  which will be parsed by Azure content understanding and then chunked into logically related sections based on headings and paragraphs.
> Once chunked, the chunks are stored in a folder and embedding is performed for vector indexing. Once embedding is complete the embedded chunks (stored in json file) are indexed in Azure AI Search.
When you attempt to index an existing document, this will be detected and ignored.

When calling the rag_this.py program, you can specify --keep_index to retain the specified index and append the document - default is False ( i.e. if you don't add the --keep_index parameter, the index will be dropped and re-created)
> The entire RAG pipeline process can be run locally on a workstation 
> build your RAG environment with: 
> python rag_this.py parameters:  sourcefile [--keep_index]

---

## Document to Markdown Converter (doc2md.py)

The `doc2md.py` utility is a standalone document conversion tool that extracts clean, structured markdown content from various document formats. It leverages Azure Content Understanding for advanced document parsing and supports multiple input formats with intelligent format detection.

### Features

#### **Multi-Format Support**
- **PDF files**: Advanced extraction using Azure Content Understanding
- **DOCX files**: Native conversion using python-docx and markdownify
- **Markdown files**: Direct copy with validation
- **Other formats**: Automatic fallback to Azure Content Understanding

#### **Azure Content Understanding Integration**
- **Intelligent document parsing**: Preserves document structure and formatting
- **Table extraction**: Maintains table structure in markdown format
- **Image handling**: Extracts and caches images from documents
- **Layout preservation**: Maintains headings, paragraphs, and document hierarchy

#### **Automated Processing**
- **Temporary analyzer creation**: Creates unique analyzers for each conversion
- **Automatic cleanup**: Removes temporary resources after processing
- **Error handling**: Comprehensive error logging and recovery
- **Batch processing ready**: Suitable for automation workflows

### Installation Requirements

Install the required dependencies:

```bash
pip install azure-identity pillow python-docx markdownify loguru
```

Ensure your Azure environment is configured with:
- Azure Content Understanding service
- Appropriate authentication (DefaultAzureCredential)
- Valid `config.json` with Azure AI endpoint configuration

### Configuration

The utility requires a `config.json` file in the parent directory with the following structure:

```json
{
    "azure_ai_endpoint": "https://your-ai-service.cognitiveservices.azure.com/",
    "azure_ai_api_version": "2024-12-01-preview"
}
```

### Usage

#### **Basic Syntax**
```bash
python doc2md.py <source_document> <target_directory>
```

#### **Parameters**
- `source_document`: Path to the input document (PDF, DOCX, MD, or other supported formats)
- `target_directory`: Directory where the output markdown file will be saved

#### **Output File Naming**
The output filename is automatically generated by appending `.md` to the source document's basename:
- Input: `document.pdf` → Output: `document.pdf.md`
- Input: `report.docx` → Output: `report.docx.md`
- Input: `existing.md` → Output: `existing.md.md`

### Usage Examples

#### **Example 1: Convert PDF to Markdown**
```bash
python doc2md.py "../documents/research_paper.pdf" "./markdown_output/"
```
**Result**: Creates `./markdown_output/research_paper.pdf.md` with structured markdown content

#### **Example 2: Convert DOCX to Markdown**
```bash
python doc2md.py "../reports/annual_report.docx" "./converted/"
```
**Result**: Creates `./converted/annual_report.docx.md` using native DOCX parsing

#### **Example 3: Copy Existing Markdown**
```bash
python doc2md.py "../content/readme.md" "./processed/"
```
**Result**: Copies the markdown file to `./processed/readme.md.md`

#### **Example 4: Batch Processing Script**
```bash
#!/bin/bash
# Convert all PDFs in a directory
for file in ../documents/*.pdf; do
    python doc2md.py "$file" "./markdown_output/"
done
```

#### **Example 5: Integration with RAG Pipeline**
```bash
# Step 1: Convert document to markdown
python doc2md.py "../source/policy_document.pdf" "./temp_md/"

# Step 2: Process with RAG system
python rag_this.py "./temp_md/policy_document.pdf.md" --keep_index
```

### Processing Behavior by File Type

#### **PDF Files (.pdf)**
- Uses Azure Content Understanding for advanced parsing
- Preserves complex layouts, tables, and document structure
- Extracts images and saves them to `.cache/` directory
- Handles multi-column layouts and complex formatting

#### **DOCX Files (.docx)**
- Uses python-docx for native document parsing
- Converts to markdown using markdownify library
- Preserves basic formatting and paragraph structure
- Faster processing compared to Azure Content Understanding

#### **Markdown Files (.md)**
- Direct file copy operation
- Validates file accessibility
- Maintains original formatting
- Instant processing

#### **Other Formats**
- Automatically uses Azure Content Understanding
- May work with PPTX, RTF, and other supported formats
- Processing success depends on Azure service capabilities

### Advanced Features

#### **Image Extraction and Caching**
```python
# Images are automatically extracted and saved
save_image(image_id, response)  # Saves to .cache/{image_id}.jpg
```

#### **Logging and Monitoring**
- Comprehensive logging to `../../logs/logs_{date}.log`
- Rotation and compression of log files
- Error tracking and debugging information
- Processing statistics and timing

#### **Error Handling**
- Graceful handling of authentication failures
- Network connectivity error recovery
- Invalid document format detection
- Resource cleanup on failure

### Troubleshooting

#### **Common Issues**

**Authentication Errors:**
```bash
# Ensure Azure CLI is logged in
az login

# Verify DefaultAzureCredential is working
az account show
```

**Missing Dependencies:**
```bash
# Install all required packages
pip install azure-identity pillow python-docx markdownify loguru
```

**Configuration Issues:**
- Verify `config.json` exists in the parent directory
- Check Azure AI endpoint URL is correct
- Ensure API version is supported

**Permission Errors:**
- Check target directory exists and is writable
- Verify source document is accessible
- Ensure sufficient disk space for output

#### **Debug Mode**
For troubleshooting, monitor the log files:
```bash
tail -f ../../logs/logs_$(date +%Y-%m-%d).log
```

### Integration with RAG System

The `doc2md.py` utility is designed to work seamlessly with the RAG pipeline:

1. **Document Conversion**: Convert source documents to clean markdown
2. **Content Preparation**: Markdown files are ideal for chunking and embedding
3. **RAG Processing**: Use converted markdown with `rag_this.py`
4. **Search Integration**: Markdown content integrates well with Azure AI Search

#### **Workflow Example**
```bash
# Convert multiple documents
python doc2md.py "policy1.pdf" "./md_files/"
python doc2md.py "policy2.docx" "./md_files/"
python doc2md.py "policy3.pdf" "./md_files/"

# Build RAG index with all converted documents
python rag_this.py "./md_files/" --keep_index
```

### Performance Considerations

#### **Processing Speed**
- **DOCX files**: Fastest (local processing)
- **Markdown files**: Instant (file copy)
- **PDF files**: Moderate (Azure API calls)
- **Complex documents**: Slower (detailed parsing)

#### **API Rate Limits**
- Azure Content Understanding has API rate limits
- Consider delays between batch processing calls
- Monitor API usage in Azure portal

#### **File Size Limits**
- PDF files: Limited by Azure Content Understanding service limits
- DOCX files: Limited by available memory
- Output size: Depends on document complexity and image content

### Best Practices

1. **Batch Processing**: Process documents in smaller batches to avoid rate limits
2. **Error Handling**: Always check log files for processing errors
3. **Storage Management**: Clean up `.cache/` directory periodically
4. **Authentication**: Ensure Azure credentials are valid before batch operations
5. **Testing**: Test with sample documents before processing large batches

---

## Token Counter Utility (token_counter.py)

The `token_counter.py` utility provides accurate token estimation for markdown files that will be used as input to OpenAI models, particularly optimized for o1 models. This tool helps you understand token consumption before making API calls, enabling better cost estimation and prompt optimization.

### Features

#### **Accurate Token Counting**
- **OpenAI-compatible tokenization**: Uses `tiktoken` library for precise token counts
- **Model-specific encoding**: Supports different tokenization schemes (o1-preview, o1-mini, gpt-4, etc.)
- **Markdown handling**: Processes markdown syntax and formatting tokens correctly
- **File size estimation**: Provides both token count and approximate API cost

#### **Multiple Output Formats**
- **Detailed analysis**: Character count, token count, and cost estimation
- **Batch processing**: Handle multiple files in a single operation
- **Summary statistics**: Overall totals for batch operations
- **Export options**: JSON output for integration with other tools

### Installation Requirements

Install the required dependencies:

```bash
pip install tiktoken
```

### Usage

#### **Basic Syntax**
```bash
python token_counter.py <markdown_file> [--model MODEL_NAME] [--cost-per-token COST] [--json]
```

#### **Parameters**
- `markdown_file`: Path to the markdown file to analyze
- `--model`: OpenAI model name for tokenization (default: "o1-preview")
- `--cost-per-token`: Cost per token in USD (default: 0.000015 for o1-preview)
- `--json`: Output results in JSON format

### Usage Examples for o1 Models

#### **Example 1: Basic Token Count for o1-preview**
```bash
python token_counter.py "./markdown_output/research_paper.pdf.md"
```
**Output:**
```
Token Analysis for research_paper.pdf.md
========================================
Characters: 15,234
Tokens (o1-preview): 3,891
Estimated Cost: $0.058
```

#### **Example 2: Token Count for o1-mini**
```bash
python token_counter.py "./converted/annual_report.docx.md" --model "o1-mini" --cost-per-token 0.000003
```
**Output:**
```
Token Analysis for annual_report.docx.md
========================================
Characters: 8,567
Tokens (o1-mini): 2,234
Estimated Cost: $0.007
```

#### **Example 3: Batch Analysis with JSON Output**
```bash
python token_counter.py "./markdown_files/*.md" --model "o1-preview" --json > token_analysis.json
```
**JSON Output:**
```json
{
  "files": [
    {
      "filename": "document1.md",
      "characters": 12345,
      "tokens": 3200,
      "estimated_cost": 0.048
    },
    {
      "filename": "document2.md", 
      "characters": 8900,
      "tokens": 2150,
      "estimated_cost": 0.032
    }
  ],
  "summary": {
    "total_files": 2,
    "total_characters": 21245,
    "total_tokens": 5350,
    "total_estimated_cost": 0.080
  }
}
```

#### **Example 4: Pre-prompt Analysis for o1 Context Window**
```bash
# Check if your converted document fits within o1's context limits
python token_counter.py "./large_document.pdf.md" --model "o1-preview"
```
**Use Case**: o1-preview has a 128K token context window. This helps ensure your document won't exceed limits.

#### **Example 5: Cost Estimation for RAG Pipeline**
```bash
# Estimate costs before processing multiple documents
for file in ./converted_docs/*.md; do
    python token_counter.py "$file" --model "o1-mini" --cost-per-token 0.000003
done
```

### Model-Specific Token Costs (2024 Rates)

#### **o1 Models**
- **o1-preview**: $0.015 per 1K input tokens
- **o1-mini**: $0.003 per 1K input tokens

#### **Usage Example with Current Rates**
```bash
# For o1-preview (production use)
python token_counter.py "assessment_content.md" --model "o1-preview" --cost-per-token 0.000015

# For o1-mini (cost-effective testing)
python token_counter.py "assessment_content.md" --model "o1-mini" --cost-per-token 0.000003
```

### Integration with Assessment Workflows

#### **Pre-Assessment Token Planning**
```bash
# Step 1: Convert assessment document
python doc2md.py "student_submission.pdf" "./temp_md/"

# Step 2: Check token usage before processing
python token_counter.py "./temp_md/student_submission.pdf.md" --model "o1-preview"

# Step 3: Proceed with assessment if within budget
if [ tokens -lt 50000 ]; then
    python auto_assess.py "./temp_md/student_submission.pdf.md"
fi
```

#### **Batch Assessment Cost Estimation**
```bash
#!/bin/bash
# Estimate costs for processing multiple student submissions
total_cost=0
for submission in ./submissions/*.pdf; do
    # Convert to markdown
    python doc2md.py "$submission" "./temp_md/"
    
    # Count tokens and get cost
    markdown_file="./temp_md/$(basename "$submission").md"
    cost=$(python token_counter.py "$markdown_file" --model "o1-mini" --json | jq '.estimated_cost')
    total_cost=$(echo "$total_cost + $cost" | bc -l)
    
    echo "Submission: $submission - Estimated cost: \$$cost"
done
echo "Total estimated cost: \$$total_cost"
```

### Advanced Features

#### **Context Window Validation**
```python
# Built-in context window limits
MODEL_LIMITS = {
    "o1-preview": 128000,
    "o1-mini": 128000,
    "gpt-4": 8192,
    "gpt-4-32k": 32768
}
```

#### **Markdown-Specific Token Handling**
- Handles markdown formatting tokens (headers, links, code blocks)
- Accounts for syntax highlighting in code sections
- Processes table formatting efficiently
- Manages image reference tokens

### Troubleshooting

#### **Common Issues**

**Large File Warnings:**
```bash
# Files approaching context limits will show warnings
Warning: Token count (125,000) approaching o1-preview limit (128,000)
```

**Model Not Found:**
```bash
# Ensure model name is correctly specified
python token_counter.py "file.md" --model "o1-preview"  # Correct
python token_counter.py "file.md" --model "o1_preview"  # Incorrect
```

**Encoding Errors:**
```bash
# Ensure file is UTF-8 encoded
file -bi filename.md  # Check encoding
iconv -f ISO-8859-1 -t UTF-8 filename.md > filename_utf8.md  # Convert if needed
```

### Performance Considerations

#### **Processing Speed**
- **Small files (<10KB)**: Instant processing
- **Medium files (10KB-1MB)**: Under 1 second
- **Large files (>1MB)**: May take several seconds

#### **Memory Usage**
- Loads entire file into memory for tokenization
- Memory usage approximately 2-3x file size
- For very large files, consider splitting before analysis

### Best Practices for o1 Integration

1. **Token Budgeting**: Always check token counts before API calls
2. **Model Selection**: Use o1-mini for testing, o1-preview for production
3. **Batch Processing**: Sum token counts across multiple documents
4. **Context Management**: Keep total tokens well below model limits (aim for 80% of limit)
5. **Cost Monitoring**: Track cumulative costs across assessment batches

---